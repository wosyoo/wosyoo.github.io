<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id/>
    <title>Guagua’s Notion 主页</title>
    <updated>2024-09-23T11:50:53.009Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <author>
        <name>学习小呆呱</name>
        <email>sywang027@gmail.com</email>
        <uri>https://wosyoo.github.io</uri>
    </author>
    <link rel="alternate" href="https://wosyoo.github.io/"/>
    <subtitle>一个NotionNext搭建的博客</subtitle>
    <icon>https://wosyoo.github.io/favicon.png</icon>
    <rights>All rights reserved 2024, 学习小呆呱</rights>
    <entry>
        <title type="html"><![CDATA[文本前端 ]]></title>
        <id>https://wosyoo.github.io/technology/text-front</id>
        <link href="https://wosyoo.github.io/technology/text-front"/>
        <updated>2024-09-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[语音合成文本前端BERT]]></summary>
        <content type="html"><![CDATA[<div id="notion-article" class="mx-auto overflow-hidden "><main class="notion light-mode notion-page notion-block-10a0c0b4c9ec80e9ab91eaa143a47efb"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-a9895ec64e254431a3b951a2e3e195d9" data-id="a9895ec64e254431a3b951a2e3e195d9"><span><div id="a9895ec64e254431a3b951a2e3e195d9" class="notion-header-anchor"></div><a class="notion-hash-link" href="#a9895ec64e254431a3b951a2e3e195d9" title="Bert-Prosody预训练文本提取文本韵律"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Bert-Prosody预训练文本提取文本韵律</span></span></h2><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-e4fa07350ef64b07a1bf0f49d52d4b8d" data-id="e4fa07350ef64b07a1bf0f49d52d4b8d"><span><div id="e4fa07350ef64b07a1bf0f49d52d4b8d" class="notion-header-anchor"></div><a class="notion-hash-link" href="#e4fa07350ef64b07a1bf0f49d52d4b8d" title="Chinese-FastSpeech2"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Chinese-FastSpeech2</span></span></h3><div class="notion-text notion-block-5c357864f67d47089cb39e5c6554d0f5">最先将Bert-Prosody用到文本前段韵律提取在：</div><a target="_blank" rel="noopener noreferrer" href="https://github.com/Executedone/Chinese-FastSpeech2" class="notion-external notion-external-block notion-row notion-block-10a0c0b4c9ec80bfaeadc78acf3f9e86"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">Chinese-FastSpeech2</div><div class="notion-external-subtitle"><span>Executedone</span><span> • </span><span>Updated <!-- -->Sep 20, 2024</span></div></div></a><div class="notion-text notion-block-10a0c0b4c9ec80d5896ae379e75e0516">以下是该模型的结构图：</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-0d47302660084064a4a09af135a07e29"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F78905e0e-76ee-4094-8d22-04cf639e7a27%2FUntitled.png?table=block&amp;id=0d473026-6008-4064-a4a0-9af135a07e29&amp;t=0d473026-6008-4064-a4a0-9af135a07e29&amp;width=4161&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-dd9b8bc4473847a99478bbf40c5b3f7d">这个项目基于FastSpeech2，在模型优化上:</div><ul class="notion-list notion-list-disc notion-block-6c92ec0ec6134ecebcbc87138adcdcc4"><li>微调了一个Prosody-Bert;</li></ul><ul class="notion-list notion-list-disc notion-block-61a4419899b942d58c78dc5d5f489380"><li>引入Prosody-Bert的文本特征，丰富Prosody Features;</li></ul><ul class="notion-list notion-list-disc notion-block-883de30b8ac3425eb5328afa12e7c4ba"><li>在Variance Adaptor中加入prosody predictor，控制韵律学习。</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-e9af595c2c954e249b6c599d7504d0e8" data-id="e9af595c2c954e249b6c599d7504d0e8"><span><div id="e9af595c2c954e249b6c599d7504d0e8" class="notion-header-anchor"></div><a class="notion-hash-link" href="#e9af595c2c954e249b6c599d7504d0e8" title="模型训练流程："><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">模型训练流程：</span></span></h4><div class="notion-text notion-block-d04f2f5bda2c49f79b030bb94f551450">采用公开的在AISHELL3上训练好的fastspeech2模型作为预训练模型，使用biaobei10000女声数据去优化模型；</div><ul class="notion-list notion-list-disc notion-block-bd4d22c869fd4023914c1539a32784b5"><li><b>阶段一（Prosody-Bert训练）</b></li><ul class="notion-list notion-list-disc notion-block-bd4d22c869fd4023914c1539a32784b5"><li>根据标贝数据的韵律标注，将每个字转成对应的韵律标签，如“卡尔普#2陪外孙#1玩滑梯#4。”，对应的标签为“011 011 011 2”，<span class="notion-orange_background">用softmax进行三分类训练</span>；</li></ul></ul><ul class="notion-list notion-list-disc notion-block-0de6a04b76e547e7a296ed1c1ad6ccd0"><li><b>阶段二（Prosody-Fastspeech2训练）</b></li><ul class="notion-list notion-list-disc notion-block-0de6a04b76e547e7a296ed1c1ad6ccd0"><li>加载预训练模型权重，同时初始化prosody predictor权重，在输入端融合prosody char embedding和phoneme embedding，按fastspeech2的方式训练声学模型；</li></ul></ul><ul class="notion-list notion-list-disc notion-block-2c653b6a6fc740108e521173110822ac"><li><b>阶段三（HifiGAN微调）</b></li><ul class="notion-list notion-list-disc notion-block-2c653b6a6fc740108e521173110822ac"><li>加载通用的HifiGAN模型，在标贝10000数据上微调。</li></ul></ul><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-b1728d7786064879a47024fc03169f37" data-id="b1728d7786064879a47024fc03169f37"><span><div id="b1728d7786064879a47024fc03169f37" class="notion-header-anchor"></div><a class="notion-hash-link" href="#b1728d7786064879a47024fc03169f37" title="BERT 预训练模型选择"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">BERT 预训练模型选择</span></span></h3><div class="notion-text notion-block-ac504cab150942b082219d76d15021a1">Chinese-FastSpeech2采用了<code class="notion-inline-code"><b>RoBERTa-wwm-ext, Chinese</b></code><b> </b>模型进行Prosody预训练.</div><div class="notion-text notion-block-457284c81f1742b58d59a87b61d15ddb">“RoBERTa-wwm-ext, Chinese”模型为哈工大讯飞开源的中文预训练模型，其名称包含了训练数据，mask方式等信息。<a target="_blank" rel="noopener noreferrer" href="https://github.com/ymcui/Chinese-BERT-wwm" class="notion-external notion-external-mention"><div class="notion-external-image"><svg viewBox="0 0 260 260"><g><path d="M128.00106,0 C57.3172926,0 0,57.3066942 0,128.00106 C0,184.555281 36.6761997,232.535542 87.534937,249.460899 C93.9320223,250.645779 96.280588,246.684165 96.280588,243.303333 C96.280588,240.251045 96.1618878,230.167899 96.106777,219.472176 C60.4967585,227.215235 52.9826207,204.369712 52.9826207,204.369712 C47.1599584,189.574598 38.770408,185.640538 38.770408,185.640538 C27.1568785,177.696113 39.6458206,177.859325 39.6458206,177.859325 C52.4993419,178.762293 59.267365,191.04987 59.267365,191.04987 C70.6837675,210.618423 89.2115753,204.961093 96.5158685,201.690482 C97.6647155,193.417512 100.981959,187.77078 104.642583,184.574357 C76.211799,181.33766 46.324819,170.362144 46.324819,121.315702 C46.324819,107.340889 51.3250588,95.9223682 59.5132437,86.9583937 C58.1842268,83.7344152 53.8029229,70.715562 60.7532354,53.0843636 C60.7532354,53.0843636 71.5019501,49.6441813 95.9626412,66.2049595 C106.172967,63.368876 117.123047,61.9465949 128.00106,61.8978432 C138.879073,61.9465949 149.837632,63.368876 160.067033,66.2049595 C184.49805,49.6441813 195.231926,53.0843636 195.231926,53.0843636 C202.199197,70.715562 197.815773,83.7344152 196.486756,86.9583937 C204.694018,95.9223682 209.660343,107.340889 209.660343,121.315702 C209.660343,170.478725 179.716133,181.303747 151.213281,184.472614 C155.80443,188.444828 159.895342,196.234518 159.895342,208.176593 C159.895342,225.303317 159.746968,239.087361 159.746968,243.303333 C159.746968,246.709601 162.05102,250.70089 168.53925,249.443941 C219.370432,232.499507 256,184.536204 256,128.00106 C256,57.3066942 198.691187,0 128.00106,0 Z M47.9405593,182.340212 C47.6586465,182.976105 46.6581745,183.166873 45.7467277,182.730227 C44.8183235,182.312656 44.2968914,181.445722 44.5978808,180.80771 C44.8734344,180.152739 45.876026,179.97045 46.8023103,180.409216 C47.7328342,180.826786 48.2627451,181.702199 47.9405593,182.340212 Z M54.2367892,187.958254 C53.6263318,188.524199 52.4329723,188.261363 51.6232682,187.366874 C50.7860088,186.474504 50.6291553,185.281144 51.2480912,184.70672 C51.8776254,184.140775 53.0349512,184.405731 53.8743302,185.298101 C54.7115892,186.201069 54.8748019,187.38595 54.2367892,187.958254 Z M58.5562413,195.146347 C57.7719732,195.691096 56.4895886,195.180261 55.6968417,194.042013 C54.9125733,192.903764 54.9125733,191.538713 55.713799,190.991845 C56.5086651,190.444977 57.7719732,190.936735 58.5753181,192.066505 C59.3574669,193.22383 59.3574669,194.58888 58.5562413,195.146347 Z M65.8613592,203.471174 C65.1597571,204.244846 63.6654083,204.03712 62.5716717,202.981538 C61.4524999,201.94927 61.1409122,200.484596 61.8446341,199.710926 C62.5547146,198.935137 64.0575422,199.15346 65.1597571,200.200564 C66.2704506,201.230712 66.6095936,202.705984 65.8613592,203.471174 Z M75.3025151,206.281542 C74.9930474,207.284134 73.553809,207.739857 72.1039724,207.313809 C70.6562556,206.875043 69.7087748,205.700761 70.0012857,204.687571 C70.302275,203.678621 71.7478721,203.20382 73.2083069,203.659543 C74.6539041,204.09619 75.6035048,205.261994 75.3025151,206.281542 Z M86.046947,207.473627 C86.0829806,208.529209 84.8535871,209.404622 83.3316829,209.4237 C81.8013,209.457614 80.563428,208.603398 80.5464708,207.564772 C80.5464708,206.498591 81.7483088,205.631657 83.2786917,205.606221 C84.8005962,205.576546 86.046947,206.424403 86.046947,207.473627 Z M96.6021471,207.069023 C96.7844366,208.099171 95.7267341,209.156872 94.215428,209.438785 C92.7295577,209.710099 91.3539086,209.074206 91.1652603,208.052538 C90.9808515,206.996955 92.0576306,205.939253 93.5413813,205.66582 C95.054807,205.402984 96.4092596,206.021919 96.6021471,207.069023 Z" fill="#161614"></path></g></svg></div><div class="notion-external-description"><div class="notion-external-title">Chinese-BERT-wwm</div><div class="notion-external-subtitle"><span>ymcui</span><span> • </span><span>Updated <!-- -->Sep 23, 2024</span></div></div></a></div><ul class="notion-list notion-list-disc notion-block-d5d1d200864143a8b251a61eeb90d416"><li>RoBERTa: RoBERTa是BERT的改进版，通过改进训练任务和数据生成方式、训练更久、使用更大批次、使用更多数据等获得了State of The Art的效果.</li></ul><ul class="notion-list notion-list-disc notion-block-38e83b35f990451b8a74a097b7fb6dc3"><li>wwm: Whole Word Masking,翻译为<code class="notion-inline-code">全词mask</code>。是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。 简单来说，原有基于WordPiece的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被mask。 在<code class="notion-inline-code">全词Mask</code>中，如果一个完整的词的部分WordPiece子词被mask，则同属该词的其他部分也会被mask，即<code class="notion-inline-code">全词Mask</code>。</li><ul class="notion-list notion-list-disc notion-block-38e83b35f990451b8a74a097b7fb6dc3"><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-56b4a6986dbe417ba9c2fa4a7baaa4ac"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F1616912e-61d0-45e3-ac5c-cbaac6f95750%2FUntitled.png?table=block&amp;id=56b4a698-6dbe-417b-a9c2-fa4a7baaa4ac&amp;t=56b4a698-6dbe-417b-a9c2-fa4a7baaa4ac&amp;width=844.9765625&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure></ul></ul><ul class="notion-list notion-list-disc notion-block-7e7316e6c6b445fc90a99e03a8ed6fb0"><li>ext: 训练数据集，EXT数据包括中文维基百科，其他百科、新闻、问答等数据，总词数达5.4B。</li></ul><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-10c39af730bc488d8c786fe05c68eca6" data-id="10c39af730bc488d8c786fe05c68eca6"><span><div id="10c39af730bc488d8c786fe05c68eca6" class="notion-header-anchor"></div><a class="notion-hash-link" href="#10c39af730bc488d8c786fe05c68eca6" title="Bert-Prosody训练细节"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Bert-Prosody训练细节</span></span></h3><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-c66b4ee266974a96a603052b1bb404b8" data-id="c66b4ee266974a96a603052b1bb404b8"><span><div id="c66b4ee266974a96a603052b1bb404b8" class="notion-header-anchor"></div><a class="notion-hash-link" href="#c66b4ee266974a96a603052b1bb404b8" title="DataLoader"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">DataLoader</span></span></h4><div class="notion-text notion-block-74628d6f50e4448f8dc878590da8a6d7">Dataset类：</div><div class="notion-text notion-block-313f80ed1def4a2c9700ca1de62dde4a">输入:文本token以及label</div><div class="notion-text notion-block-a876a60dfd9b49ac92f4df427ff772ac">输出：</div><ul class="notion-list notion-list-disc notion-block-f98b1c7205b34992b42130cd0b6bb706"><li>inputs_ids: 文本token编码的结果，一个batch内长度一样，长度不一致末尾补0</li></ul><ul class="notion-list notion-list-disc notion-block-0335bc04d76249ff9700b25f3c05dd65"><li>inputs_masks: 掩码，有数据为1，没数据为0</li></ul><ul class="notion-list notion-list-disc notion-block-627912316fd84a66b48b578d42ca7112"><li>tokens_type_ids: 全0</li></ul><ul class="notion-list notion-list-disc notion-block-b4ae45899dfe4581ada7f66f6ccbe3c5"><li>label: 三分类韵律标签</li></ul><div class="notion-text notion-block-9ea86f8f09bb4563a2e6bd8a5da9ab81">以下是一个文本输入的实例(batch_size=2)：</div><div class="notion-text notion-block-90260c22b9d24ce7baaa5d378f416334">labels来源：</div><div class="notion-text notion-block-2ccc22942dc9421596315f360b1fbc6f">根据单句分词结果，分块编号。每个块起始位置编号都是0，非起始位置编号为1.句中遇到标点符号编号为2.(所有单句的末尾都含有句号)</div><div class="notion-text notion-block-1dfd84834ba74e09a5e9a84f9d31fce6">示例：</div><div class="notion-text notion-block-e2b6eaeddf774178b94166a668a8cd1c">&quot;text&quot;: &quot;眼眶宽阔而低矮，鼻短而宽。&quot;,</div><div class="notion-text notion-block-1e666806c4564efa9d9b843894ae8444">分词结果：             [眼眶   宽阔   而   低矮   ，   鼻短   而宽   。]</div><div class="notion-text notion-block-9fb462fb90214a35b099f88818eec2b5"> &quot;prosody_label&quot;:  [0, 1,    0, 1,   0,   0, 1,  2,    0, 1,    0, 1,  2  ]</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-57da7a1fa5154ee2b06a6ad25a2e2efb" data-id="57da7a1fa5154ee2b06a6ad25a2e2efb"><span><div id="57da7a1fa5154ee2b06a6ad25a2e2efb" class="notion-header-anchor"></div><a class="notion-hash-link" href="#57da7a1fa5154ee2b06a6ad25a2e2efb" title="Prosody Model"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Prosody Model</span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-ff13b38a1bac48718da8a0e3c945c7f2"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:672px;max-width:100%;flex-direction:column"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F9d17d415-4805-4db2-a68e-7f5e98be138a%2FUntitled.png?table=block&amp;id=ff13b38a-1bac-4871-8da8-a0e3c945c7f2&amp;t=ff13b38a-1bac-4871-8da8-a0e3c945c7f2&amp;width=672&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-b496a7fe8c8b4cd2843ee548fe899843"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:576px;max-width:100%;flex-direction:column"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F16d6a6ba-191a-43d8-95e5-746e31948f04%2FUntitled.png?table=block&amp;id=b496a7fe-8c8b-4cd2-843e-e548fe899843&amp;t=b496a7fe-8c8b-4cd2-843e-e548fe899843&amp;width=576&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-f271d9989d274c0d8e772dd8851be463">最终每个字符token都得到一个1*3的向量，用这个向量和labels做交叉熵损失，训练文本的韵律。</div><div class="notion-text notion-block-d0f58e0d79ca43578103d70e846c4f88">训练时参数更新只更新bert后面的两个线性层。</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-edc1ed29e63b415c899d3111d2d9069f" data-id="edc1ed29e63b415c899d3111d2d9069f"><span><div id="edc1ed29e63b415c899d3111d2d9069f" class="notion-header-anchor"></div><a class="notion-hash-link" href="#edc1ed29e63b415c899d3111d2d9069f" title="如何使用训练好的BERT-Prosody？"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">如何使用训练好的BERT-Prosody？</span></span></h4><div class="notion-text notion-block-c71516b15a1c40a7a3db17ea1d4363ef">在训练声学模型时，prosody和文本phoneme embedding concat一起作为先验知识送入模型中。但是此时不使用ProsodyModel的第二个Linear Layer，只使用第一个，即生成的tensor大小为N*max_len*256.</div><div class="notion-blank notion-block-c5ffb87ce27c4ac19bbf34c4bfcc8763"> </div></main></div>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Transformer]]></title>
        <id>https://wosyoo.github.io/technology/vit</id>
        <link href="https://wosyoo.github.io/technology/vit"/>
        <updated>2024-09-23T00:00:00.000Z</updated>
        <content type="html"><![CDATA[<div id="notion-article" class="mx-auto overflow-hidden "><main class="notion light-mode notion-page notion-block-fff0c0b4c9ec819aaa3ad8a27fbc8807"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><div class="notion-blank notion-block-fff0c0b4c9ec816b8c9ad3b21497a8b9"> </div></main></div>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer]]></title>
        <id>https://wosyoo.github.io/technology/transformer</id>
        <link href="https://wosyoo.github.io/technology/transformer"/>
        <updated>2024-09-23T00:00:00.000Z</updated>
        <content type="html"><![CDATA[<div id="notion-article" class="mx-auto overflow-hidden "><main class="notion light-mode notion-page notion-block-fff0c0b4c9ec8170a3dbe2247a972ea3"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><div class="notion-text notion-block-fff0c0b4c9ec81b590acd04bb6c13841"><b>参考资料：</b></div><div class="notion-text notion-block-fff0c0b4c9ec81638623d55b15aac046"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://zhuanlan.zhihu.com/p/338817680">Transformer模型详解（图解最完整版） - 知乎 (zhihu.com)</a></div><div class="notion-text notion-block-fff0c0b4c9ec81fcb4fbda33f127472e"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.bilibili.com/video/BV1o44y1Y7cp/?spm_id_from=333.788&amp;vd_source=3c8b956b7f9637a100dfb9d16c7b8d5d">18、深入剖析PyTorch中的Transformer API源码_哔哩哔哩_bilibili</a></div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec819bb287c397fa4b1fb1"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:432px;max-width:100%;flex-direction:column"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F661e070b-4f54-46f8-80bf-db24ba6d8c84%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-819b-b287-c397fa4b1fb1&amp;t=fff0c0b4-c9ec-819b-b287-c397fa4b1fb1&amp;width=432&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec810990eadbfdbf0e9292"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:1296px;max-width:100%;flex-direction:column"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F14d2cddf-847f-4297-9e1e-bb4bdedd7222%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8109-90ea-dbfdbf0e9292&amp;t=fff0c0b4-c9ec-8109-90ea-dbfdbf0e9292&amp;width=1296&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81a58b83ca2c8a4aa3c0"><b>transformer pytorch api使用：</b></div><div class="notion-row"><a target="_blank" rel="noopener noreferrer" class="notion-bookmark notion-block-fff0c0b4c9ec8190b341f6aded0d1da3" href="https://blog.csdn.net/zhaohongfei_358/article/details/126019181"><div><div class="notion-bookmark-title">Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解_iioSnail的博客-CSDN博客</div><div class="notion-bookmark-description">1. Transformer的训练过程讲解2. Transformer的推理过程讲解3. Transformer的入参和出参讲解4. nn.Transformer的各个参数讲解5. nn.Transformer的mask机制详解6. 实战：使用nn.Transformer训练一个copy任务。_nn.transformer</div><div class="notion-bookmark-link"><div class="notion-bookmark-link-icon"><img src="https://www.notion.so/image/https%3A%2F%2Fg.csdnimg.cn%2Fstatic%2Flogo%2Ffavicon32.ico?table=block&amp;id=fff0c0b4-c9ec-8190-b341-f6aded0d1da3&amp;t=fff0c0b4-c9ec-8190-b341-f6aded0d1da3" alt="Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解_iioSnail的博客-CSDN博客" loading="lazy" decoding="async"/></div><div class="notion-bookmark-link-text">https://blog.csdn.net/zhaohongfei_358/article/details/126019181</div></div></div></a></div><div class="notion-blank notion-block-fff0c0b4c9ec813f826ff308f8d967ca"> </div></main></div>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAE]]></title>
        <id>https://wosyoo.github.io/technology/vae</id>
        <link href="https://wosyoo.github.io/technology/vae"/>
        <updated>2024-09-23T00:00:00.000Z</updated>
        <content type="html"><![CDATA[<div id="notion-article" class="mx-auto overflow-hidden "><main class="notion light-mode notion-page notion-full-width notion-block-fff0c0b4c9ec8102a8dac18ae9a78bb1"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81aa8854f8f28d8c0c01"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F2eb2cef1-8f73-4663-b4e4-cf1cbf4287cd%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81aa-8854-f8f28d8c0c01&amp;t=fff0c0b4-c9ec-81aa-8854-f8f28d8c0c01&amp;width=2062&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec817f9413f53ecd3b248f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F6c3e45cb-a045-4d63-8b6f-7f4699472d23%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-817f-9413-f53ecd3b248f&amp;t=fff0c0b4-c9ec-817f-9413-f53ecd3b248f&amp;width=519.9765625&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8164bc4cd086d7bd9aca"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F87412ddd-2baf-4782-957b-a231908b8703%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8164-bc4c-d086d7bd9aca&amp;t=fff0c0b4-c9ec-8164-bc4c-d086d7bd9aca&amp;width=519.9453125&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-blank notion-block-fff0c0b4c9ec8101a0aacfe4935227ed"> </div></main></div>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion model]]></title>
        <id>https://wosyoo.github.io/technology/diffusion</id>
        <link href="https://wosyoo.github.io/technology/diffusion"/>
        <updated>2024-09-23T00:00:00.000Z</updated>
        <content type="html"><![CDATA[<div id="notion-article" class="mx-auto overflow-hidden "><main class="notion light-mode notion-page notion-block-fff0c0b4c9ec813fa9f5d6ee51d583f4"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><div class="notion-text notion-block-fff0c0b4c9ec81598e5ad5b88b03e164">Denoising Diffusion Probabilistic Models简称Diffusion model.</div><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-fff0c0b4c9ec81b3a6c3ed1076c5e3c7" data-id="fff0c0b4c9ec81b3a6c3ed1076c5e3c7"><span><div id="fff0c0b4c9ec81b3a6c3ed1076c5e3c7" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec81b3a6c3ed1076c5e3c7" title="Diffusion Model概念"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Diffusion Model概念</span></span></h3><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-fff0c0b4c9ec81de9675ebadc1adddab" data-id="fff0c0b4c9ec81de9675ebadc1adddab"><span><div id="fff0c0b4c9ec81de9675ebadc1adddab" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec81de9675ebadc1adddab" title="Diffusion model如何运作"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Diffusion model如何运作</span></span></h4><div class="notion-text notion-block-fff0c0b4c9ec81ecb816d873da4e5fa9">假如我们要生成一张100*100的图像，那么我们先从一个正态分布中采样100*100个pixel，然后进行denoise去噪声化一步步生成最终清晰的图片，这个过程叫做<b>reverse process</b>.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81018aaedc0ca77989fb"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F1cbc4a81-4c0d-401d-a97b-4b0f05e5b992%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8101-8aae-dc0ca77989fb&amp;t=fff0c0b4-c9ec-8101-8aae-dc0ca77989fb&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81de9f17e7a29b319d2a">我们会复用这个denoise model，每一步生成的图片都送入到一个denoise model去除噪声。但是由于每个阶段的图片noise的程度是不同的，所以我们要在denoise model输入加上step(这个step其实是含噪声的程度，由大到小)。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81a2be30cb081b41aaa2"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F40a4f995-39a7-4c34-99a0-2244b944ab4e%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81a2-be30-cb081b41aaa2&amp;t=fff0c0b4-c9ec-81a2-be30-cb081b41aaa2&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec817fb65cdfb09622b940"><b>denoise model里面到底长啥样？</b></div><div class="notion-text notion-block-fff0c0b4c9ec81b99494ca6e17cd4c3e">denoise model包含两个输入，其中一个为图片，另一个为step。里面包含一个noise predictor，用来预测输入图片的噪声，noise predictor会输出一个预测这个图片的noise，那么我们在原图上减去这个noise就能得到去除噪声后的图片。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec815cb683f5628f253d86"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F8ac73da0-b0d1-4b8a-a2dc-467d4a613c7c%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-815c-b683-f5628f253d86&amp;t=fff0c0b4-c9ec-815c-b683-f5628f253d86&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81b39de5d6ed19ddee2e">也有一些denoise model直接产生去除噪声后的图片。</div><div class="notion-text notion-block-fff0c0b4c9ec8173b323d00f67a0eb91">那怎样训练noise predictor呢？怎样获取一张图片中的噪声部分呢？</div><div class="notion-text notion-block-fff0c0b4c9ec815bb77cf9496e6606fe">这里有一个forward process(diffusion process).也就是不断的给原始图像加噪声的过程。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81c9ba55f3bacab4d10f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Fe101b20b-077d-4af4-af85-fbc5b757f2b2%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81c9-ba55-f3bacab4d10f&amp;t=fff0c0b4-c9ec-81c9-ba55-f3bacab4d10f&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81038433c2290f7c6b17">将原始图片一步一步加上噪声的过程就是forward process，将过程中产生的input image和step id作为input，将加入的noise作为ground truth训练noise predictor.</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-fff0c0b4c9ec81d891d9e5bc90ba2f52" data-id="fff0c0b4c9ec81d891d9e5bc90ba2f52"><span><div id="fff0c0b4c9ec81d891d9e5bc90ba2f52" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec81d891d9e5bc90ba2f52" title="Text-to-Image"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Text-to-Image</span></span></h4><div class="notion-text notion-block-fff0c0b4c9ec81baae32f6e9c1d6c9f6">训练文生图模型需要pairs of text-image图片，大型的文生图模型要非常大量的图片。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec818f9dc1e2e2b203d1df"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F36c64cfc-fe38-474d-8c53-4ca7fae39310%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-818f-9dc1-e2e2b203d1df&amp;t=fff0c0b4-c9ec-818f-9dc1-e2e2b203d1df&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81e49b55e4f20eed82c4">LAION包含58.5亿张图片。</div><div class="notion-text notion-block-fff0c0b4c9ec81cc81a0d75b2200378c">到现在也没有用上text，那么怎样利用diffusion model文生图呢？</div><div class="notion-text notion-block-fff0c0b4c9ec812f87eaddb76f00ece5">在denoise model中加入额外的text信息。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8112b965f7e4293dc66f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Fa47324f6-7baa-461f-88a8-99279fd6412b%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8112-b965-f7e4293dc66f&amp;t=fff0c0b4-c9ec-8112-b965-f7e4293dc66f&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec814fa9bcd656e8f002e3">也就是直接把text输入到noise predictor.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8150a959c701ca67a8c9"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F7db4b71e-05c1-4400-8590-a1c7d15cdb5b%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8150-a959-c701ca67a8c9&amp;t=fff0c0b4-c9ec-8150-a959-c701ca67a8c9&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-fff0c0b4c9ec81b3960bc2b9f12c384b" data-id="fff0c0b4c9ec81b3960bc2b9f12c384b"><span><div id="fff0c0b4c9ec81b3960bc2b9f12c384b" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec81b3960bc2b9f12c384b" title="Stable Diffusion/DALLE/Imagen 共同的套路"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Stable Diffusion/DALLE/Imagen 共同的套路</span></span></h3><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-fff0c0b4c9ec8172bb51cff6e3e164c9" data-id="fff0c0b4c9ec8172bb51cff6e3e164c9"><span><div id="fff0c0b4c9ec8172bb51cff6e3e164c9" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec8172bb51cff6e3e164c9" title="Framework"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Framework</span></span></h4><div class="notion-text notion-block-fff0c0b4c9ec810fa70df1104aef6d9b">现在主流的文生图模型都是下面的结构：</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81bea544d4a2068550b4"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F8f3c3f33-7141-496e-83ca-29d705567f5f%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81be-a544-d4a2068550b4&amp;t=fff0c0b4-c9ec-81be-a544-d4a2068550b4&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81f5801de2292ddbff7b">text经过text encoder得到向量表示；噪声和text表征向量一起输入到Generation model得到一个图片的压缩表示，最后输入到decoder得到图片。</div><div class="notion-text notion-block-fff0c0b4c9ec81f5adbed4b5caeea0c2">Stable Diffusion/DALLE/Imagen采用的都是类似的结构。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec810f8d95df8402857567"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F4a61c8b9-30ff-4710-8e86-d605cf0b5f92%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-810f-8d95-df8402857567&amp;t=fff0c0b4-c9ec-810f-8d95-df8402857567&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec811eacf1c17634aa2bfe"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F349c4981-8f6d-41d4-bb3e-d08ce5217bd5%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-811e-acf1-c17634aa2bfe&amp;t=fff0c0b4-c9ec-811e-acf1-c17634aa2bfe&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8144bc2ce5fc44b3dc06"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Febc2cb50-f514-4a14-b72f-a4fbe4498513%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8144-bc2c-e5fc44b3dc06&amp;t=fff0c0b4-c9ec-8144-bc2c-e5fc44b3dc06&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81af9a58c3efeb0a6be7">下面介绍framework的三个部件。</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-fff0c0b4c9ec8117aacac5c37e85d98c" data-id="fff0c0b4c9ec8117aacac5c37e85d98c"><span><div id="fff0c0b4c9ec8117aacac5c37e85d98c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec8117aacac5c37e85d98c" title="Text Encoder"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Text Encoder</span></span></h4><div class="notion-text notion-block-fff0c0b4c9ec81878c2ed526a286c585">可以使用GPT Bert等作为文本编码模型。</div><div class="notion-text notion-block-fff0c0b4c9ec818bbac6cc6d753703da">text encoder对于生成图片的质量非常重要。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec811ea928ec668953c58e"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Faf8b64bd-dc19-4f36-9de1-90c1487255b6%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-811e-a928-ec668953c58e&amp;t=fff0c0b4-c9ec-811e-a928-ec668953c58e&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81a6a8c2c09dfe90b9d0">上图中越往右下角表示效果越好，可见encoder size对于模型效果影响较大，而diffusion model的size影响较小。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81c9bd27cd61e4510f8d"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F2675e0db-8aa5-427f-a9c5-8ea5aba428b0%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81c9-bd27-cd61e4510f8d&amp;t=fff0c0b4-c9ec-81c9-bd27-cd61e4510f8d&amp;width=520&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec811d8288dc9d57dc776e">上图中FID是指将图像输入到一个CNN之后得到的分布之间的距离。这里假设generative和real的分布均为高斯分布。所以FID越小表示生成的效果越好。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec812884dbc68ef974b8a4"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Fbaf6afff-9d8b-4d08-94d7-2b83313fa736%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8128-84db-c68ef974b8a4&amp;t=fff0c0b4-c9ec-8128-84db-c68ef974b8a4&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81929107fb59969ccbc8">CLIP指标计算文本和图像之间的相似程度。训练时将标注好的image-text pairs输入到模型中，让训练集中成对描述之间距离小，否则距离大。那么在计算CLIP Score时，如果输入的文本和图片很接近，说明score大。所以CLIP Score越大越好。</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-fff0c0b4c9ec814e9f3be2ef5656e813" data-id="fff0c0b4c9ec814e9f3be2ef5656e813"><span><div id="fff0c0b4c9ec814e9f3be2ef5656e813" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec814e9f3be2ef5656e813" title="Decoder"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Decoder</span></span></h4><div class="notion-text notion-block-fff0c0b4c9ec8186a53cc31d2d13a63f">decoder训练不需要大量的text-image pairs, 训练时只需要图片。</div><div class="notion-text notion-block-fff0c0b4c9ec81b4b8b1cef4b51877fc">但是训练decoder需要中间产物(latent representation)作为输入，所以我们需要一个auto-encoder.输入图片得到图片的latent representation.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81c1b18ac560b6d8dc5f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F1a737ad9-ab59-4ec6-843f-76131319d3b8%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81c1-b18a-c560b6d8dc5f&amp;t=fff0c0b4-c9ec-81c1-b18a-c560b6d8dc5f&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-fff0c0b4c9ec8142928fff8612316817" data-id="fff0c0b4c9ec8142928fff8612316817"><span><div id="fff0c0b4c9ec8142928fff8612316817" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec8142928fff8612316817" title="Generative Model"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Generative Model</span></span></h4><div class="notion-text notion-block-fff0c0b4c9ec8150bfcdc0d6fad86d58">这个model和上面diffusion model不一样的地方在于这里输入的图片都是latent representation.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81d18df5c4adec17100e"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Fbf2b2eb7-ac4c-4cbc-a2ef-94404d01971c%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81d1-8df5-c4adec17100e&amp;t=fff0c0b4-c9ec-81d1-8df5-c4adec17100e&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81359edcfc5f1cdedfd1">首先将图片输入到一个encoder中得到latent representation，然后经过diffusion process一步步得到噪声图。</div><div class="notion-text notion-block-fff0c0b4c9ec8126bd58e392e77118c2">将diffusion process的结果作为训练数据训练noise predictor.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81a3b94bc9db9100c9dc"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Fcecb8043-fcc3-470e-854a-96d2fcb13af7%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81a3-b94b-c9db9100c9dc&amp;t=fff0c0b4-c9ec-81a3-b94b-c9db9100c9dc&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec813c87bff74e194e904c">在最后通过reverse process产生图片。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8115a90eccfbdeb5909e"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Fb90a24e7-22ac-42d3-aef1-2061c5550f35%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8115-a90e-ccfbdeb5909e&amp;t=fff0c0b4-c9ec-8115-a90e-ccfbdeb5909e&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-fff0c0b4c9ec816ea326c5ff71dcb06b" data-id="fff0c0b4c9ec816ea326c5ff71dcb06b"><span><div id="fff0c0b4c9ec816ea326c5ff71dcb06b" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec816ea326c5ff71dcb06b" title="Diffusion Model数学原理"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Diffusion Model数学原理</span></span></h3><div class="notion-text notion-block-fff0c0b4c9ec81258191f41002aaa6a3">实际上在DDPM的原始论文里，Diffusion model的训练过程和上面讲到的原理部分还是有些不同的。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec818badfbfbd85a0f56ee"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Fe6e687de-c6bf-47cc-99f1-9fd36df9b91f%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-818b-adfb-fbd85a0f56ee&amp;t=fff0c0b4-c9ec-818b-adfb-fbd85a0f56ee&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec8166b364e8cc695e2916">上图为训练的流程图：</div><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec81d48426d9496c0ba6c2"><li>首先我们需要从原始的clean image分布中采样一张clean image出来，记为<!-- -->;</li></ul><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec815eb02bf45317cc1ff5"><li>然后从1~T中取一个值t;</li></ul><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec81549cb5cdee55c9e098"><li>然后从标准正态分布中采样出噪声ε;</li></ul><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec81f7a3d6f48ba92edbc6"><li>对于公式<!-- -->求梯度，即最小化该公式。</li><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec81f7a3d6f48ba92edbc6"><li>公式中后半部分<!-- -->实际上就是noise predictor，他接受两个输入，分别是noisy image和t；</li><li>，<!-- -->，···，<!-- -->都是提前给出的数字。T越大，<!-- -->越小，所以原图<!-- -->比例越小，noise 比例越大。</li><li>所以综上，上述公式就是让noise predictor产生的预测noise越接近真实noise越好。</li></ul></ul><div class="notion-text notion-block-fff0c0b4c9ec8191862ddb5adb9990bf">下图更加直观的展示了训练过程。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8199b046d367bac353a2"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2Fbc16dd44-fc36-4868-ae2d-4654e4f890d1%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8199-b046-d367bac353a2&amp;t=fff0c0b4-c9ec-8199-b046-d367bac353a2&amp;width=519.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81418046ce2314e4bbd7">所以实际上我们之前预想的DDPM的训练过程和实际上的流程是不太一样的：</div><div class="notion-text notion-block-fff0c0b4c9ec8125a957f6594f51b795">我们一开始认为noise是一步一步被加上的，但是noise实际上是一次被加上。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81a09f32d2674a0d461d"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F2aa38077-64d6-44ad-b0b6-2150da971990%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81a0-9f32-d2674a0d461d&amp;t=fff0c0b4-c9ec-81a0-9f32-d2674a0d461d&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81fc9549d0cb22c1b79c">下面再来看一下inference的步骤：</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81758ab8ee78f8c8a565"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fprod-files-secure.s3.us-west-2.amazonaws.com%2Fe86941ae-3a6f-453b-b9e8-0a7f1f519ea0%2F26f90632-e4db-4cd2-9356-69d6719c387b%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8175-8ab8-ee78f8c8a565&amp;t=fff0c0b4-c9ec-8175-8ab8-ee78f8c8a565&amp;width=519.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81fb9294e26af6bdc79b">上面的流程图很好的展示了这个过程，但是会有一点让人不解的地方在于为什么在最后还要加上一个noise z???</div><div class="notion-text notion-block-fff0c0b4c9ec81d39b5eead2a0abcac1">这一部分DDPM论文中并没有给出解释，李宏毅老师的课程中给出了比较合理的解释，我们在得到一个目标distribution后往往不能取均值，其中一个原因是如果每次都取均值，那么我们模型每次生成的结果都是一样的，例如文生图和语言模型，模型缺乏泛化性；另外一个原因是我们人类在正常产生文本时并不是每一步都采用的是概率最大的结果，所以模型需要从分布中采样，而不是直接取概率最大的均值。一个非常实际的应用就是tacotron模型在inference时也使用了dropout，增加随机性。
另外，diffusion model实际上是一个auto-regressive运用到non-autoregressive的实例，diffusion process实际上可以采用一步实现。在这之前也有一些结合autoregressive和non-autoregressive的方法，Mask-predict就是先利用non-autoregressive产生结果，再mask掉结果中概率小的部分，重新生成结果，一步一步得到最终结果。这样的方法比autoregressive时间复杂度小，但是比non-autoregressive效果好，这可能是未来研究的一个比较好的方向。</div><div class="notion-blank notion-block-fff0c0b4c9ec8153874fd2730709f1ac"> </div></main></div>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flow-based Generative model]]></title>
        <id>https://wosyoo.github.io/technology/flow</id>
        <link href="https://wosyoo.github.io/technology/flow"/>
        <updated>2024-09-23T00:00:00.000Z</updated>
        <content type="html"><![CDATA[<div id="notion-article" class="mx-auto overflow-hidden "><main class="notion light-mode notion-page notion-block-fff0c0b4c9ec8197b935f4879f5122b3"><div class="notion-viewport"></div><div class="notion-collection-page-properties"></div><div class="notion-text notion-block-fff0c0b4c9ec810db31be874a6747607">资料来源：</div><div class="notion-row"><a target="_blank" rel="noopener noreferrer" class="notion-bookmark notion-block-fff0c0b4c9ec8173878df535d92a6c63" href="https://0809zheng.github.io/2022/05/01/flow.html"><div><div class="notion-bookmark-title">流模型(Flow-based Model) - 郑之杰的个人网站</div><div class="notion-bookmark-description">为天地立心, 为生民立命, 为往圣继绝学, 为万世开太平</div><div class="notion-bookmark-link"><div class="notion-bookmark-link-icon"><img src="https://www.notion.so/image/https%3A%2F%2F0809zheng.github.io%2Ffavicon.ico?table=block&amp;id=fff0c0b4-c9ec-8173-878d-f535d92a6c63&amp;t=fff0c0b4-c9ec-8173-878d-f535d92a6c63" alt="流模型(Flow-based Model) - 郑之杰的个人网站" loading="lazy" decoding="async"/></div><div class="notion-bookmark-link-text">https://0809zheng.github.io/2022/05/01/flow.html</div></div></div></a></div><div class="notion-row"><a target="_blank" rel="noopener noreferrer" class="notion-bookmark notion-block-fff0c0b4c9ec81c5a03df2e02c0c5f5a" href="https://www.youtube.com/watch?v=uXY18nzdSsM"><div><div class="notion-bookmark-title">Flow-based  Generative Model</div><div class="notion-bookmark-link"><div class="notion-bookmark-link-icon"><img src="https://www.notion.so/image/https%3A%2F%2Fwww.youtube.com%2Fs%2Fdesktop%2F11fc5992%2Fimg%2Ffavicon_144x144.png?table=block&amp;id=fff0c0b4-c9ec-81c5-a03d-f2e02c0c5f5a&amp;t=fff0c0b4-c9ec-81c5-a03d-f2e02c0c5f5a" alt="Flow-based  Generative Model" loading="lazy" decoding="async"/></div><div class="notion-bookmark-link-text">https://www.youtube.com/watch?v=uXY18nzdSsM</div></div></div><div class="notion-bookmark-image"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fi.ytimg.com%2Fvi%2FuXY18nzdSsM%2Fhqdefault.jpg?table=block&amp;id=fff0c0b4-c9ec-81c5-a03d-f2e02c0c5f5a&amp;t=fff0c0b4-c9ec-81c5-a03d-f2e02c0c5f5a" alt="Flow-based  Generative Model" loading="lazy" decoding="async"/></div></a></div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-fff0c0b4c9ec812eb2e5e1ac5fc2909f" data-id="fff0c0b4c9ec812eb2e5e1ac5fc2909f"><span><div id="fff0c0b4c9ec812eb2e5e1ac5fc2909f" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec812eb2e5e1ac5fc2909f" title="目录"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">目录</span></span></h2><div class="notion-table-of-contents notion-gray notion-block-fff0c0b4c9ec8194926fc9f45a60b794"><a href="#fff0c0b4c9ec812eb2e5e1ac5fc2909f" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">目录</span></a><a href="#fff0c0b4c9ec81a89ff9ff1c234590a0" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">前面生成模型存在的问题</span></a><a href="#fff0c0b4c9ec817eb9a4d40447f4e8f8" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">Generator</span></a><a href="#fff0c0b4c9ec81919d91d70bf8abb166" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">Math Background</span></a><a href="#fff0c0b4c9ec81c39397cc067dda7e58" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:48px">Jacobian Matrix</span></a><a href="#fff0c0b4c9ec81a39f03eefb11897736" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:48px">Determinant(行列式)</span></a><a href="#fff0c0b4c9ec811e931ec19099d0a524" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:48px">Change of Variable Theorem(变量替换定理)</span></a><a href="#fff0c0b4c9ec816ab3afc086b1afac3b" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">Flow-based Model</span></a><a href="#fff0c0b4c9ec81a8a0e8db6fc7d48a4f" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:48px">Coupling layer</span></a><a href="#fff0c0b4c9ec8135b1e0d554b52677cd" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:48px">Coupling layer - stacking</span></a><a href="#fff0c0b4c9ec814baabcf9d0fac22bb5" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">Glow</span></a></div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-fff0c0b4c9ec81a89ff9ff1c234590a0" data-id="fff0c0b4c9ec81a89ff9ff1c234590a0"><span><div id="fff0c0b4c9ec81a89ff9ff1c234590a0" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec81a89ff9ff1c234590a0" title="前面生成模型存在的问题"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">前面生成模型存在的问题</span></span></h3><div class="notion-text notion-block-fff0c0b4c9ec81279996d4891abe84ce">1.Auto-regressive model</div><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec8166b247df2c3d734182"><li>对于图像来说，不知道生成每个像素的顺序是什么</li></ul><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec8125b90affadf9473bc6"><li>一步一步生成，速度太慢</li></ul><div class="notion-text notion-block-fff0c0b4c9ec8177b420e7c2ec011805">2.VAE</div><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec81cf8fb5c0d6d41a9de7"><li>优化对象不是似然，而是似然的下界</li></ul><div class="notion-text notion-block-fff0c0b4c9ec81b49f24d6e85f44034e">3.GAN</div><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec813bb3a1fcee8d181e32"><li>训练不稳定/难以训练</li></ul><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-fff0c0b4c9ec817eb9a4d40447f4e8f8" data-id="fff0c0b4c9ec817eb9a4d40447f4e8f8"><span><div id="fff0c0b4c9ec817eb9a4d40447f4e8f8" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec817eb9a4d40447f4e8f8" title="Generator"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Generator</span></span></h3><ul class="notion-list notion-list-disc notion-block-fff0c0b4c9ec8119bec4cc14a48ae2eb"><li>A generator G is a network. The network defines a probability distribution </li></ul><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8134af94ebb672496c9c"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6784061c-fc35-4ad0-a10d-402476e28fa0%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8134-af94-ebb672496c9c&amp;t=fff0c0b4-c9ec-8134-af94-ebb672496c9c&amp;width=707.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec811d9dcac975ed01b33d">由于G是一个network，所以<!-- -->显然非常的复杂，一般不知道怎么最大化似然函数，但是flow可以直接优化似然函数！</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-fff0c0b4c9ec81919d91d70bf8abb166" data-id="fff0c0b4c9ec81919d91d70bf8abb166"><span><div id="fff0c0b4c9ec81919d91d70bf8abb166" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec81919d91d70bf8abb166" title="Math Background"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Math Background</span></span></h3><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-fff0c0b4c9ec81c39397cc067dda7e58" data-id="fff0c0b4c9ec81c39397cc067dda7e58"><span><div id="fff0c0b4c9ec81c39397cc067dda7e58" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec81c39397cc067dda7e58" title="Jacobian Matrix"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Jacobian Matrix</span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8144adf9e3b99ba24a64"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0f3d9d6e-c691-4974-8473-61c6b739da48%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8144-adf9-e3b99ba24a64&amp;t=fff0c0b4-c9ec-8144-adf9-e3b99ba24a64&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec8137bab1d4571fc04ce5">对于输入z，函数f会产生输出x。那么函数f的Jacobian Matrix就是上图的<!-- -->, 注意这个矩阵的顺序关系是不能改变的。f的反函数就是<!-- -->, 计算可以得到<!-- -->的Jacobian Matrix如图。那么有如下性质：<!-- -->(单位矩阵)</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-fff0c0b4c9ec81a39f03eefb11897736" data-id="fff0c0b4c9ec81a39f03eefb11897736"><span><div id="fff0c0b4c9ec81a39f03eefb11897736" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec81a39f03eefb11897736" title="Determinant(行列式)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Determinant(行列式)</span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec818baa9cfe5d689d76ab"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1d61a3f2-14dd-4e2a-9786-63525ff135e6%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-818b-aa9c-fe5d689d76ab&amp;t=fff0c0b4-c9ec-818b-aa9c-fe5d689d76ab&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81e0b0d7cb4c74c1bf4f">|Det(A)|实际上计算的是以高维空间以方阵A每一行为坐标点围成形状的“体积”。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec818fb786ff89b45e2b5e"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6190059c-1dda-485f-add1-87f05a824b91%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-818f-b786-ff89b45e2b5e&amp;t=fff0c0b4-c9ec-818f-b786-ff89b45e2b5e&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-fff0c0b4c9ec811e931ec19099d0a524" data-id="fff0c0b4c9ec811e931ec19099d0a524"><span><div id="fff0c0b4c9ec811e931ec19099d0a524" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec811e931ec19099d0a524" title="Change of Variable Theorem(变量替换定理)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Change of Variable Theorem(变量替换定理)</span></span></h4><div class="notion-text notion-block-fff0c0b4c9ec814f82d1cae003b3fe91">如果我们有一个原始的分布π(z), 通过一个变换函数x=f(z), 得到的x任然是一个分布p(x)，那么这两个分布p(x)和π(z)有什么关系呢？</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81b18d6afa2129d60242"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa6b48429-b39b-4a00-9ae6-efa2a953fad4%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81b1-8d6a-fa2129d60242&amp;t=fff0c0b4-c9ec-81b1-8d6a-fa2129d60242&amp;width=707.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81639bd6cea478debaa9">假如z是一个01均匀分布，那么z的概率密度函数如图π(z)所示。由于密度函数积分为1，所以正方形高为1.</div><div class="notion-text notion-block-fff0c0b4c9ec81ec81e2d19fd779f5a6">x=2z+1, 那么x的密度函数取值范围就是[1,3]，那么绿色正方形高就是0.5.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81bc995dc8e600165dea"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F0f4386cd-dd44-4598-ba7c-71dfd73d486b%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81bc-995d-c8e600165dea&amp;t=fff0c0b4-c9ec-81bc-995d-c8e600165dea&amp;width=707.984375&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81419d4fe5eb1cbbf48e">那么我们推广到一般情况，我们不知道z的分布和x的分布。我们在<!-- -->到∆z之间取值，那么经过变换函数x=f(z), 这一段x的取值记为<!-- -->到∆x.</div><div class="notion-text notion-block-fff0c0b4c9ec81f99ccbfa43521fb90b">在这一部分概率密度函数应该有相同的面积，所以可以由图中公式得出结论。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8194b231e2264dab8ca0"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fd2c6830a-b21b-4df8-b850-3ea052f6bec6%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8194-b231-e2264dab8ca0&amp;t=fff0c0b4-c9ec-8194-b231-e2264dab8ca0&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81039220ff8627fb8fbe">推广到二维和多维情况：</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8181ab40f88381204d35"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff089bfd3-3aad-45ce-85f3-d60309720ef3%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8181-ab40-f88381204d35&amp;t=fff0c0b4-c9ec-8181-ab40-f88381204d35&amp;width=707.9921875&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec817582acfca46cda47b3">对于二维情况，我们同样计算两个部分的面积，他们的面积是相等的。其中<!-- -->表示<!-- -->改变<!-- -->时，x_{1}的改变量，<!-- -->表示<!-- -->改变<!-- -->时，<!-- -->的改变量.</div><div class="notion-text notion-block-fff0c0b4c9ec81d6804ec76e4dd184af">那么我们可以逐步推导得到下面的结果：</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81b4b36eff7235adce24"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb4966d30-5d4f-4113-b4de-140d1daf2590%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81b4-b36e-ff7235adce24&amp;t=fff0c0b4-c9ec-81b4-b36e-ff7235adce24&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-fff0c0b4c9ec816ab3afc086b1afac3b" data-id="fff0c0b4c9ec816ab3afc086b1afac3b"><span><div id="fff0c0b4c9ec816ab3afc086b1afac3b" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec816ab3afc086b1afac3b" title="Flow-based Model"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Flow-based Model</span></span></h3><div class="notion-text notion-block-fff0c0b4c9ec81a3a646f13a8cc9b798">有了上面的推导结果，我们就可以把Generator的原始分布和生成分布之间的关系写出来，如下图所示。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81d7b4e0e276181d296d"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb17dd56d-01f6-434b-afa8-f3044176d550%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81d7-b4e0-e276181d296d&amp;t=fff0c0b4-c9ec-81d7-b4e0-e276181d296d&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81a1a7bdceef51dce7cf">为了计算<!-- -->, 我们要保证能够计算出生成器的det(<!-- -->). 由于在输入和输出很多维的情况下，Jacobian Matrix会很大，计算量巨大，所以要认真设计Generator的结构。为了保证<!-- -->是可以计算出来的(G是可逆的)，我们要保证输入输出的channel是一致的。如果input z和output x的维度不一致，那么G一定不是可逆的。维度一致不一定可逆。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81c5bc09f6d4678ca1c8"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F16c2093a-ee83-45bf-b2a5-98f4a1f8192b%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81c5-bc09-f6d4678ca1c8&amp;t=fff0c0b4-c9ec-81c5-bc09-f6d4678ca1c8&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81de96badd96e75b073d">由于G的结构是受限制的，所以G的表达能力也是有限的。所以Flow使用了多个G.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec814e9905c5656c3598bc"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fd928de4a-bdec-4d5e-b8cd-4494361bb3ca%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-814e-9905-c5656c3598bc&amp;t=fff0c0b4-c9ec-814e-9905-c5656c3598bc&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81a6b8efde227abf3509">那么我们实际上是怎样训练的呢？</div><div class="notion-text notion-block-fff0c0b4c9ec8193bc8bdc8b2288f1a6">以单个G为例：</div><div class="notion-text notion-block-fff0c0b4c9ec818dacb1d289ddf21970">由于我们最大化的目标函数只包含<!-- -->, 而不包含G，所以我们在训练时实际上是训练<!-- -->，然后用G做生成任务。训练时，我们从目标分布中采样<!-- -->, 送到<!-- -->，得到<!-- -->. z的初始化分布为标准正态分布，所以目标函数的第一项会让<!-- -->尽量生成0矩阵，这样loss最小，这样显然不行。如果z一直是零矩阵，不那么雅戈比行列式就会是0，取log就是负无穷，这样就无法最大化目标函数。于是<!-- -->会折中，让生成的z尽量向0靠近，但是不能是零矩阵。</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-fff0c0b4c9ec81a8a0e8db6fc7d48a4f" data-id="fff0c0b4c9ec81a8a0e8db6fc7d48a4f"><span><div id="fff0c0b4c9ec81a8a0e8db6fc7d48a4f" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec81a8a0e8db6fc7d48a4f" title="Coupling layer"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Coupling layer</span></span></h4><div class="notion-text notion-block-fff0c0b4c9ec81488337c6e7ae94fb5a">G的结构应该怎么样设才能满足上面提到的要求呢，一种设计是Coupling Layer, 如下图所示。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81b1bdb0fb822eeef9e1"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6f3e109f-2f01-4da0-bb6b-9b4ec519edd5%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81b1-bdb0-fb822eeef9e1&amp;t=fff0c0b4-c9ec-81b1-bdb0-fb822eeef9e1&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec8101a20ccb394bb4b910">我们把输入的z分割为两个部分，前一个部分直接复制得到前一个部分的x；然后将前一部分做两个变换，这连个变换任意，多复杂都行，使其能够生成和后一部分长度相同的序列。然后后一部分x的计算如图所示，其中⊙表示的是按位乘运算。</div><div class="notion-text notion-block-fff0c0b4c9ec81679d3dd04815dafb6d">这样设计G能够容易得到G的逆。G_{-1}计算如下图所示：</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec817195a2f514c2e885e0"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4adacb99-1f08-4a57-b458-6abd7dcc9fb7%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8171-95a2-f514c2e885e0&amp;t=fff0c0b4-c9ec-8171-95a2-f514c2e885e0&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec8124bccfec94521a7133">现在我们还需要计算J(G).Coupling Layer的J也非常好计算.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81009702c295c1cc0eec"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc813d751-b262-421d-939b-ade393353848%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8100-9702-c295c1cc0eec&amp;t=fff0c0b4-c9ec-8100-9702-c295c1cc0eec&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec8194a10ff3e0ec459aa2">由于<!-- -->直接由<!-- -->复制得到，所以上图阴影部分左上部分为单位矩阵。由于<!-- -->的变化与<!-- -->无关, 所以阴影部分右上部分为0.所以整个det(J(G))只和阴影部分右下角有关。阴影部分右下角由上图公式可以算出是一个对角阵，行列式的值就是对角元素乘积，也就是<!-- -->.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-fff0c0b4c9ec8135b1e0d554b52677cd" data-id="fff0c0b4c9ec8135b1e0d554b52677cd"><span><div id="fff0c0b4c9ec8135b1e0d554b52677cd" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec8135b1e0d554b52677cd" title="Coupling layer - stacking"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Coupling layer - stacking</span></span></h4><div class="notion-text notion-block-fff0c0b4c9ec818c8a47ee0e48f5414f">当我们叠加多个G时就会出现问题。上面部分会一直复制到最后一层。一个简单的解决方案就是把G上下颠倒。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec818b9cc8f9d67934f923"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fecb5a14f-8a3f-4cb3-8070-a352036c3309%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-818b-9cc8-f9d67934f923&amp;t=fff0c0b4-c9ec-818b-9cc8-f9d67934f923&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec817e82e4c98d1ece80bc">例如在CV中，可以采用单数coupling，双数transform；或者在通道上选择若干通道coupling，若干通道transform。</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-fff0c0b4c9ec814baabcf9d0fac22bb5" data-id="fff0c0b4c9ec814baabcf9d0fac22bb5"><span><div id="fff0c0b4c9ec814baabcf9d0fac22bb5" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fff0c0b4c9ec814baabcf9d0fac22bb5" title="Glow"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Glow</span></span></h3><div class="notion-text notion-block-fff0c0b4c9ec8118beb0cbf760e704a1">G使用了1*1 convolution.</div><div class="notion-text notion-block-fff0c0b4c9ec81f5bdbeff854791fe31">通俗直观理解：1*1卷积可以交换通道位置，相当于上面stack的方式由机器自己学习。</div><div class="notion-text notion-block-fff0c0b4c9ec815a8848c82498586aca">那么如果卷积的kernel矩阵是可逆的，G就是可逆的。但是W是学习出来的，它一定可逆吗？？不知道，我们只在初始化kernel时设定为一个可逆的矩阵。</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec813c9780c3bd25f8f488"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb09b8227-7135-4559-9f26-487511712465%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-813c-9780-c3bd25f8f488&amp;t=fff0c0b4-c9ec-813c-9780-c3bd25f8f488&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec81649353ec9f40ebcd29">那么这个G的Jacobian Matrix是什么呢？</div><div class="notion-text notion-block-fff0c0b4c9ec8124a3bddc8c276a7ecc">答案：就是这个kernel matrix.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec819999abe4651e662e89"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F07299c05-4fda-42d3-8d7c-8a802d991dec%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8199-99ab-e4651e662e89&amp;t=fff0c0b4-c9ec-8199-99ab-e4651e662e89&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81b19c57fa90334aa09e"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3593290a-2cb5-4ed9-bc87-bb83f6b517cf%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81b1-9c57-fa90334aa09e&amp;t=fff0c0b4-c9ec-81b1-9c57-fa90334aa09e&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec812592deca47a706876b"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F85216d70-ee30-4cfd-84b2-c58245170d55%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8125-92de-ca47a706876b&amp;t=fff0c0b4-c9ec-8125-92de-ca47a706876b&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec8181ac09c856d4e80255"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F36088758-8561-4606-a30d-087a118b072f%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-8181-ac09-c856d4e80255&amp;t=fff0c0b4-c9ec-8181-ac09-c856d4e80255&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-text notion-block-fff0c0b4c9ec8195879ce47c24e564ab">在语音合成应用：</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fff0c0b4c9ec81c2bacce72654c3fa03"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column;height:100%"><img style="object-fit:cover" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4a1e7aa4-6132-4cfd-82b0-b2e50327e2e4%2FUntitled.png?table=block&amp;id=fff0c0b4-c9ec-81c2-bacc-e72654c3fa03&amp;t=fff0c0b4-c9ec-81c2-bacc-e72654c3fa03&amp;width=2132&amp;cache=v2" alt="notion image" loading="lazy" decoding="async"/></div></figure><div class="notion-blank notion-block-fff0c0b4c9ec81149f26cf20f5622dd1"> </div></main></div>]]></content>
    </entry>
</feed>